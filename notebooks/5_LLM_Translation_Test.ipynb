{"cells":[{"cell_type":"code","execution_count":null,"id":"01ec2ace-756a-4e55-8f05-92605ef42628","metadata":{"id":"01ec2ace-756a-4e55-8f05-92605ef42628"},"outputs":[],"source":[]},{"cell_type":"code","source":["!pip install -U pip && \\\n","  pip install -U datasets evaluate sentencepiece transformers wandb scikit-learn nltk rouge-score accelerate && \\\n","  pip install -U flash-attn --no-build-isolation optimum && \\\n","  pip install -U bitsandbytes>=0.39.0 accelerate>=0.20.0"],"metadata":{"id":"CSKkVSWhjTX_"},"id":"CSKkVSWhjTX_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"314fbbb3-a467-489f-85a3-33534d57be0f","metadata":{"id":"314fbbb3-a467-489f-85a3-33534d57be0f"},"outputs":[],"source":["!(cd Lucrare-de-dizertatie-2024/ && PYTHONPATH=. python3 dizertatie/main.py)"]},{"cell_type":"code","execution_count":null,"id":"a9e6eeae-e3ea-4130-9c79-959d647e75e1","metadata":{"id":"a9e6eeae-e3ea-4130-9c79-959d647e75e1"},"outputs":[],"source":["import sys\n","sys.path.append('/root/Lucrare-de-dizertatie-2024/')"]},{"cell_type":"code","execution_count":null,"id":"9ae3195d-1c74-4304-b9a4-643e223f7eaa","metadata":{"id":"9ae3195d-1c74-4304-b9a4-643e223f7eaa"},"outputs":[],"source":["import dizertatie\n","import pathlib\n","from dizertatie.configs.common import PROJECT_SEED\n","from dizertatie.dataset.dataset import DatasetConfig, load\n","\n","DATA_PATH = pathlib.Path('/root/Lucrare-de-dizertatie-2024/data')\n","\n","ro_sent = load(DatasetConfig(\n","    shuffle_seed=PROJECT_SEED,\n","    subsample_size=None,\n","    path=DATA_PATH\n","), 'RoSent')"]},{"cell_type":"code","execution_count":null,"id":"e994ab4b-b4cb-4220-9a36-5877a88a85dc","metadata":{"id":"e994ab4b-b4cb-4220-9a36-5877a88a85dc"},"outputs":[],"source":["ro_sent"]},{"cell_type":"markdown","id":"05c5c0eb-fdce-4b79-a9e2-a324df131be2","metadata":{"id":"05c5c0eb-fdce-4b79-a9e2-a324df131be2"},"source":["## Help links\n","* https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/discussions/36#65b8d5cf23d948d884d19645\n","* https://huggingface.co/docs/transformers/perf_infer_gpu_one\n","* https://huggingface.co/docs/transformers/perf_train_gpu_one\n","* https://huggingface.co/docs/transformers/perf_train_gpu_many\n","* https://huggingface.co/docs/transformers/big_models#low-memory-loading\n","* https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file\n","* https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\n","* https://discuss.huggingface.co/t/model-inference-on-tokenized-dataset/14820\n","* https://chat.lmsys.org/\n","\n","* https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\n","* https://huggingface.co/openchat/openchat-3.5-0106\n","* https://huggingface.co/TheBloke/claude2-alpaca-13B-GGUF\n","* https://huggingface.co/TheBloke/Wizard-Vicuna-30B-Uncensored-GPTQ\n","* https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\n","* https://huggingface.co/state-spaces/mamba-2.8b\n","\n","* https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n","\n","* https://huggingface.co/docs/transformers/chat_templating\n","* https://huggingface.co/docs/transformers/perf_infer_gpu_one\n","* https://huggingface.co/docs/transformers/perf_torch_compile\n","* https://huggingface.co/docs/transformers/llm_tutorial#generate-text\n","\n","* https://medium.com/@mayvic/llm-multi-gpu-batch-inference-with-accelerate-edadbef3e239\n","* https://huggingface.co/docs/accelerate/usage_guides/distributed_inference"]},{"cell_type":"code","execution_count":null,"id":"1cc95e1d-5cbc-492d-89b2-1c4e8dfa5f95","metadata":{"id":"1cc95e1d-5cbc-492d-89b2-1c4e8dfa5f95"},"outputs":[],"source":["!pip install -U flash-attn --no-build-isolation optimum"]},{"cell_type":"code","execution_count":null,"id":"5d8649f8-356f-40ef-ab34-03ae6cf832a0","metadata":{"id":"5d8649f8-356f-40ef-ab34-03ae6cf832a0"},"outputs":[],"source":["!pip install -U bitsandbytes>=0.39.0 accelerate>=0.20.0 optimum onnxruntime onnx"]},{"cell_type":"code","execution_count":null,"id":"2eb11dd4-1d8d-43f9-9357-c61e5e53b94a","metadata":{"id":"2eb11dd4-1d8d-43f9-9357-c61e5e53b94a"},"outputs":[],"source":["MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\""]},{"cell_type":"code","execution_count":null,"id":"4db546b6-dc06-40c0-979b-d0e0a2e60ad0","metadata":{"id":"4db546b6-dc06-40c0-979b-d0e0a2e60ad0"},"outputs":[],"source":["# not supported\n","# import torch\n","# from optimum.onnxruntime import ORTModelForCausalLM\n","\n","# model = ORTModelForCausalLM.from_pretrained(\n","#   MODEL_NAME,\n","#   export=True,\n","#   provider=\"CUDAExecutionProvider\",\n","# )\n","# model = torch.compile(model)"]},{"cell_type":"code","execution_count":null,"id":"ce4d4101-8d73-4500-a696-06c2f5857b37","metadata":{"id":"ce4d4101-8d73-4500-a696-06c2f5857b37"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import torch\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n","model = AutoModelForCausalLM.from_pretrained(\n","    MODEL_NAME, device_map=\"auto\",\n","    # attn_implementation=\"flash_attention_2\",\n","    # torch_dtype=torch.bfloat16,\n",")\n","model = torch.compile(model)"]},{"cell_type":"code","execution_count":null,"id":"2d423b55-c910-4aaa-a2de-357dc2b94a73","metadata":{"id":"2d423b55-c910-4aaa-a2de-357dc2b94a73"},"outputs":[],"source":["!free -h && nvidia-smi"]},{"cell_type":"code","execution_count":null,"id":"5e7751c2-2fe4-400c-91ef-3bf3b41dd092","metadata":{"id":"5e7751c2-2fe4-400c-91ef-3bf3b41dd092"},"outputs":[],"source":["model.device"]},{"cell_type":"code","execution_count":null,"id":"e49f32cb-d420-4259-b61c-66d90959b7b6","metadata":{"id":"e49f32cb-d420-4259-b61c-66d90959b7b6"},"outputs":[],"source":["tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"id":"d8485384-19c8-4905-a62d-2a15ba062d3c","metadata":{"id":"d8485384-19c8-4905-a62d-2a15ba062d3c"},"outputs":[],"source":["import html\n","import re\n","\n","N = 270\n","M = 360\n","\n","def make_template(text):\n","    text = prep_text(text)\n","    text = tokenizer.encode(text, padding=False, truncation=True, max_length=N)\n","    text = tokenizer.decode(text, skip_special_tokens=True)\n","\n","    return [{\n","        'role': 'user', 'content': f\"\"\"You are a helpful professional translator. You will be prompted with texts to translate. You will respond only with the translation.\n","You will receive prompts with the format: \"Translate from Romanian to English: [Romanian text]\".\n","You will respond with: \"Translation: [English text].\n","Translate from Romanian to English: {text}\"\"\"\n","    }]\n","\n","def prep_text(x):\n","    x = re.sub(r'\\s+', html.unescape(x).replace('\\\\', '\\\\\\\\'), ' ').strip()[:30719].replace('\\\\\\\\', '\\\\').strip().replace('\\n', ' ')\n","    return \"un produs interesant, nici bun, nici rau\" if x == \"\" else x\n","\n","def mistral_tokenize(examples):\n","    templates = list(map(make_template, examples['text_ro']))\n","\n","    result = list(\n","        map(\n","            lambda x: tokenizer.apply_chat_template(x, tokenize=False).replace('<s>', '', 1),\n","            templates\n","        )\n","    )\n","    result = tokenizer(result, padding='max_length', truncation=False, max_length=M)\n","    # print(tokenizer.batch_decode(result['input_ids']))\n","\n","    examples['input_ids'] = result['input_ids']\n","    examples['attention_mask'] = result['attention_mask']\n","\n","    return examples\n","\n","ro_sent_tokenized = ro_sent.map(mistral_tokenize, batched=True).remove_columns(['text_ro', 'target']).with_format('torch')"]},{"cell_type":"code","execution_count":null,"id":"fb52cb13-8627-4666-8a06-0cbd853732b4","metadata":{"id":"fb52cb13-8627-4666-8a06-0cbd853732b4"},"outputs":[],"source":["[x[-5:] for x in ro_sent_tokenized[:10]['input_ids']], ro_sent_tokenized['input_ids'].shape"]},{"cell_type":"code","execution_count":null,"id":"a2755362-df8c-4b39-8aa9-5b976cbf85fe","metadata":{"id":"a2755362-df8c-4b39-8aa9-5b976cbf85fe"},"outputs":[],"source":["ro_sent_tokenized"]},{"cell_type":"code","execution_count":null,"id":"d0ce30ca-3f34-4de6-86b2-f53a2b811ee9","metadata":{"id":"d0ce30ca-3f34-4de6-86b2-f53a2b811ee9"},"outputs":[],"source":["import gc\n","# del model\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"id":"aa254f14-668d-4d19-b763-f44d60b3ff92","metadata":{"id":"aa254f14-668d-4d19-b763-f44d60b3ff92"},"outputs":[],"source":["# !mkdir -p mistral_ro_sent"]},{"cell_type":"code","execution_count":null,"id":"f3c15125-cd7e-48f4-a58e-1fe1a3b392e3","metadata":{"id":"f3c15125-cd7e-48f4-a58e-1fe1a3b392e3"},"outputs":[],"source":["for x in torch.utils.data.DataLoader(ro_sent_tokenized, batch_size = 86, shuffle=False):\n","    print(x['id'])\n","    print(tokenizer.decode(x['input_ids'][0]))\n","    break"]},{"cell_type":"code","execution_count":null,"id":"ecbfeb37-c865-400a-b224-2a66dfc501b5","metadata":{"id":"ecbfeb37-c865-400a-b224-2a66dfc501b5"},"outputs":[],"source":["%%time\n","\n","import tqdm\n","import json\n","\n","loader = torch.utils.data.DataLoader(\n","    ro_sent_tokenized.remove_columns(['id']), batch_size = 1,\n","    shuffle=False,\n","    pin_memory=True,\n","    num_workers=4\n",")\n","\n","SEP_TOKEN = '[/INST]'\n","ANS_PREFIX = 'Translation:'\n","\n","with torch.no_grad():\n","    for batch_idx, batch in enumerate(tqdm.tqdm(loader)):\n","        inputs = {k: v.cuda() for k, v in batch.items()}\n","\n","        generated_ids = model.generate(\n","            **inputs, max_new_tokens=int(N+0.2), pad_token_id=tokenizer.pad_token_id, do_sample=True,\n","            temperature=0.7, top_p=1 # settings from https://chat.lmsys.org/\n","        )\n","        decoded = tokenizer.batch_decode(generated_ids)\n","\n","        for i, v in enumerate(decoded):\n","            start = v.index('<s>')\n","            stop = v.index('</s>', start)\n","            v = v[start:stop].replace('<s>', '').replace('</s>', '')\n","\n","            separator = v.index(SEP_TOKEN)\n","            prompt = v[:separator].replace('[INST]', '').strip()\n","            answer = v[separator+len(SEP_TOKEN):].strip()\n","            if answer.startswith(ANS_PREFIX):\n","                answer = answer[len(ANS_PREFIX):]\n","\n","            try:\n","                answer = answer[:answer.index(ANS_PREFIX)].strip()\n","            except:\n","                pass\n","\n","            answer = answer.strip()\n","\n","            # print(\"Prompt:\", prompt)\n","            # print(\"###\")\n","            # print(\"Answer:\", answer)\n","            # print(\"====================\")\n","\n","            decoded[i] = answer\n","\n","        with open(f'mistral_ro_sent/batch_{batch_idx}', 'w') as f:\n","            json.dump(decoded, f)\n","\n","        del inputs\n","        if (batch_idx)%50==0:\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","        break"]},{"cell_type":"code","execution_count":null,"id":"25e02fa0-240a-4e8e-9a06-bbd140d76eaf","metadata":{"id":"25e02fa0-240a-4e8e-9a06-bbd140d76eaf"},"outputs":[],"source":["del inputs"]},{"cell_type":"code","execution_count":null,"id":"8a475c7e-9c25-4697-88a4-6185a1451407","metadata":{"id":"8a475c7e-9c25-4697-88a4-6185a1451407"},"outputs":[],"source":["!python infer_mistral.py"]},{"cell_type":"code","execution_count":null,"id":"1331a1be-8d68-4b71-9423-fe2dd434eb02","metadata":{"id":"1331a1be-8d68-4b71-9423-fe2dd434eb02"},"outputs":[],"source":["!nvidia-smi"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}